import os
import time
import pickle

import matplotlib.pyplot as plt

from utils.image_utils import *
from utils.mpi_utils import get_similarity

def clean_key_names(d):
    new_d = {}
    for k,v in d.items():
        new_d[k.split('/')[-1].split('.')[0]] = v
    return new_d

def get_n_models():
    return len([i for i in os.listdir('/mnt/home/users/tic_163_uma/manuzagra/output/') if not os.path.isfile('/mnt/home/users/tic_163_uma/manuzagra/output/'+i)])

images_pkl = 'results/images.pkl'
try:
    with open(images_pkl, 'rb') as f:
        ground_truth, model_images = pickle.load(f)
    
    print(f'Images read from the file "{images_pkl}"')
except:
    tic = time.time()

    n_images = 0

    # the one generated by gt
    ground_truth = get_images('/mnt/home/users/tic_163_uma/manuzagra/output/exe_data_random-0_*/runs/evaluation/scene/rendered_val/*.png_gt.png')
    ground_truth = clean_key_names(ground_truth)

    # get all the images from out models
    # number 0 is the original one
    model_images = {}

    # number of folders
    n_models = get_n_models()
    for model in range(n_models+1):
        model_images[model] = get_images(f'/mnt/home/users/tic_163_uma/manuzagra/output/exe_data_random-{model}_*/runs/evaluation/scene/rendered_val/*.png_ours.png')
        model_images[model] = clean_key_names(model_images[model])
        n_images += len(model_images[model])

    print(f'{n_images} images read in {time.time()-tic} secs.')
    
    # save the metrics
    with open(images_pkl, 'wb') as f:
        pickle.dump((ground_truth, model_images), f)
    
    print(f'Images serialized and saved as "{images_pkl}"')


## calculate the metrics for all the images
# metrics = {
#     'image_name': {
#         'model_name': {
#             'metric_name': metric
#         }
#     }
# }

metrics_pkl = 'results/metrics.pkl'
try:
    with open(metrics_pkl, 'rb') as f:
        metrics = pickle.load(f)
    
    print(f'Metrics read from the file "{metrics_pkl}"')
except:
    tic = time.time()

    metrics = {}

    ## calculate the similarity of the other models
    for model in model_images.keys():
        metrics[model] = {}
        for img in model_images[model].keys():
            try:
                print(f'Calculando model {model}, image {img} ...')
                metrics[model][img] = get_similarity(ground_truth[img], model_images[model][img])
            except:
                print(f'Model {model}, image {img} failed.')
    
    print(f'Metrics calculated in {time.time()-tic} secs.')

    tic = time.time()

    # calculate the average performance of each model
    for model in metrics.keys():
        model_av_metrics = {'PSNR': 0, 'SSIM': 0, 'LPIPS': 0}
        n_img = len(metrics[model])
        for img in metrics[model].keys():
            for metric in metrics[model][img].keys():
                model_av_metrics[metric] += metrics[model][img][metric] / n_img
        metrics[model]['av'] = dict(model_av_metrics)
    
    print(f'Average calculated in {time.time()-tic} secs.')
    
    # save the metrics
    with open(metrics_pkl, 'wb') as f:
        pickle.dump(metrics, f)
    
    print(f'Metrics serialized and saved as "{metrics_pkl}"')



multi_images_pkl = 'results/multi_images.pkl'
multi_metrics_pkl = 'results/multi_metrics.pkl'
try:
    with open(multi_images_pkl, 'rb') as f:
        multi_model_images = pickle.load(f)

    with open(multi_metrics_pkl, 'rb') as f:
        multi_model_metrics = pickle.load(f)
    
    
    print(f'Images read from the file "{multi_images_pkl}"')
    print(f'Metrics read from the file "{multi_metrics_pkl}"')
except:
    tic = time.time()

    # lets sort the models by their SSIM (for example)
    sorted_models = list(metrics.keys())
    sorted_models.sort(key=lambda model: metrics[model]['av']['SSIM'], reverse=True)


    # now we are going to get some average images using different number of images
    # as the list is sorted we will use always the images with better score
    multi_model_images = {}
    multi_model_metrics = {}
    
    n_models = get_n_models()
    for i in range(2, n_models+1):
        models = sorted_models[:i]
        models_name = '-'.join([str(i) for i in models])

        multi_model_images[models_name] = {}
        multi_model_metrics[models_name] = {}

        for img in ground_truth.keys():
            imgs = []
            for model in models:
                imgs.append(model_images[model][img])
            multi_model_images[models_name][img] = average_image(imgs)
            multi_model_metrics[models_name][img] = get_similarity(ground_truth[img], multi_model_images[models_name][img])

            # save the image
            plt.imsave(f"results/av_image_{models_name}_{img}.png", multi_model_images[models_name][img].astype(np.uint8)[:,:,::-1])
    

    tic = time.time()

    # calculate the average performance of each model
    for model in multi_model_metrics.keys():
        model_av_metrics = {'PSNR': 0, 'SSIM': 0, 'LPIPS': 0}
        n_img = len(multi_model_metrics[model])
        for img in multi_model_metrics[model].keys():
            for metric in multi_model_metrics[model][img].keys():
                model_av_metrics[metric] += multi_model_metrics[model][img][metric] / n_img
        multi_model_metrics[model]['av'] = dict(model_av_metrics)
    
    print(f'Average calculated in {time.time()-tic} secs.')
    
    # save the images
    with open(multi_images_pkl, 'wb') as f:
        pickle.dump(multi_model_images, f)
    # save the metrics
    with open(multi_metrics_pkl, 'wb') as f:
        pickle.dump(multi_model_metrics, f)
    
    print(f'Images serialized and saved as "{multi_images_pkl}"')
    
    print(f'Metrics serialized and saved as "{multi_metrics_pkl}"')






x = []
y_PSNR = []
y_SSIM = []
y_LPIPS = []
for i, model in enumerate(multi_model_metrics.keys()):
    x.append(i+2)
    y_PSNR.append(multi_model_metrics[model]['av']['PSNR'])
    y_SSIM.append(multi_model_metrics[model]['av']['SSIM'])
    y_LPIPS.append(multi_model_metrics[model]['av']['LPIPS'])


fig, axs = plt.subplots( nrows=3, ncols=1, sharex=True)

axs[0].plot(x, y_PSNR, color='r', linestyle='dashdot')
axs[0].axhline(metrics[0]['av']['PSNR'], color='r')
axs[0].set_ylabel('PSNR')
axs[0].legend(['PSNR', 'PSNR-referencia'], loc=7, bbox_to_anchor=(1.35,0.5))

axs[1].plot(x, y_SSIM, color='b', linestyle='dashdot')
axs[1].axhline(metrics[0]['av']['SSIM'], color='b')
axs[1].set_ylabel('SSIM')
axs[1].legend(['SSIM', 'SSIM-referencia'], loc=7, bbox_to_anchor=(1.35,0.5))

axs[2].plot(x, y_LPIPS, color='g', linestyle='dashdot')
axs[2].axhline(metrics[0]['av']['LPIPS'], color='g')
axs[2].set_ylabel('LPIPS')
axs[2].legend(['LPIPS', 'LPIPS-referencia'], loc=7, bbox_to_anchor=(1.35,0.5))

axs[2].set_xlabel('Numero de modelos agregados')

# ax.set_xlabel('Number of images used')
# ax.set_ylabel('Similarity')

axs[0].set_xticks(range(min(x), max(x)+1, 1))

fig.savefig('results/av_graph.png', bbox_inches='tight')
# fig.savefig('results/av_image.png')
plt.close(fig)
